<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ApnaOS – Memory Management</title>
  <link rel="stylesheet" href="css/style.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
</head>
<body>
  <div class="sidebar">
    <a href="index.html"><i class="fas fa-house"></i> Home</a>
    <a href="kernel.html"><i class="fas fa-cogs"></i> Kernel</a>
    <a href="process.html"><i class="fas fa-tasks"></i> Process</a>
    <a href="syscalls.html"><i class="fas fa-code"></i> System Calls</a>
    <a href="cli.html"><i class="fas fa-terminal"></i> CLI</a>
    <a href="interrupts.html"><i class="fas fa-bolt"></i> Interrupts</a>
    <a href="memory.html" class="active"><i class="fas fa-microchip"></i> Memory</a>
    <a href="filesystem.html"><i class="fas fa-folder"></i> File System</a>
  </div>

  <div class="container">
    <header class="site-header">
      <h1>Memory Management in ApnaOS</h1>
      <p>Exploring both the theoretical foundations and the detailed implementation of our kernel's memory subsystem.</p>
    </header>

    <main>
      <article>

        <!-- Conceptual Foundations -->
        <section>
          <h2>1. Conceptual Foundations</h2>
          <p>
            At the heart of ApnaOS’s memory management lies a <strong>bump allocator</strong> (also known as a linear or stack allocator). This strategy reserves a contiguous region of memory and serves allocation requests by simply advancing a pointer (the “bump” pointer), without immediately reclaiming freed blocks. While extremely fast—requiring only pointer arithmetic per allocation—it does not handle fragmentation or reuse freed space.
          </p>
          <p>
            To keep track of individual allocations, each block is prefixed with a small <code>memory_block_t</code> header storing metadata:<ul>
            <li><strong>size</strong>: the length of the user-requested payload.</li>
            <li><strong>is_free</strong>: a flag indicating whether the block has been released.</li>
            </ul>
          </p>
          <p>
            Beyond byte-level allocations, operating systems commonly allocate whole pages (4 KB) to manage paging structures and map virtual memory. ApnaOS wraps its bump allocator to serve page-granularity requests. Finally, when forking or cloning processes, the kernel must duplicate page tables. Although production-grade kernels use copy-on-write to delay copying, our initial design performs a straightforward memory copy of the page table root.
          </p>
        </section>

        <!-- Implementation Details -->
        <section>
          <h2>2. Implementation Details</h2>

          <h3>2.1 Memory Pool Initialization</h3>
          <p>
            We reserve a static 16 MB buffer for kernel allocations:
          </p>
          <pre><code>#define KERNEL_MEMORY_SIZE (1024 * 1024 * 16)  // 16 MB
static uint8_t kernel_memory[KERNEL_MEMORY_SIZE];
static uint32_t kernel_memory_offset = 0;
          </code></pre>
          <p>
            During <code>memory_init()</code>, we clear this buffer to avoid exposing residual data and reset the bump pointer (<code>kernel_memory_offset</code>) to zero. This sets the stage for subsequent <code>kmalloc</code> calls to carve out memory from the start of the buffer.
          </p>
          <pre><code>void memory_init(uint32_t multiboot_info) {
    // Zero-out the entire pool
    for (uint32_t i = 0; i < KERNEL_MEMORY_SIZE; i++) {
        kernel_memory[i] = 0;
    }
    kernel_memory_offset = 0;
    debug_print("DEBUG: Kernel memory initialized.");
    // Future: parse multiboot_info to build page tables, bitmap, etc.
}
          </code></pre>

          <h3>2.2 Byte-level Allocation (<code>kmalloc</code>)</h3>
          <p>
            To satisfy allocation requests of <code>size</code> bytes, <code>kmalloc</code> performs the following steps:
          </p>
          <ol>
            <li><strong>Compute total size:</strong> add <code>sizeof(memory_block_t)</code> to accommodate the header.</li>
            <li><strong>Bounds check:</strong> ensure <code>kernel_memory_offset + total</code> does not exceed our pool.</li>
            <li><strong>Header placement:</strong> write <code>size</code> and clear <code>is_free</code> at the bump pointer.</li>
            <li><strong>Offset bumping:</strong> advance by <code>total</code> bytes, then align <code>kernel_memory_offset</code> up to the next 4-byte boundary to satisfy alignment requirements on many architectures.</li>
            <li><strong>Return payload:</strong> pointer arithmetic yields an address just past the header.
            </li>
          </ol>
          <pre><code>void* kmalloc(size_t size) {
    size_t total = size + sizeof(memory_block_t);
    if (kernel_memory_offset + total > KERNEL_MEMORY_SIZE) {
        return NULL; // insufficient space
    }

    // Place header at bump pointer
    memory_block_t* block = (memory_block_t*)&kernel_memory[kernel_memory_offset];
    block->size = size;
    block->is_free = 0;

    // Advance offset
    kernel_memory_offset += total;
    // Align to 4-byte boundary
    if (kernel_memory_offset % 4 != 0) {
        kernel_memory_offset += (4 - (kernel_memory_offset % 4));
    }

    // Return pointer to payload region
    return (void*)(block + 1);
}
          </code></pre>

          <h3>2.3 Deallocation (<code>kfree</code>)</h3>
          <p>
            Although our bump allocator does not reclaim space on free, we update the header’s <code>is_free</code> flag for each block. This bookkeeping will enable a future free-list or coalescing allocator to identify available blocks:
          </p>
          <pre><code>void kfree(void* ptr) {
    if (!ptr) return;
    // Locate header immediately before payload
    memory_block_t* block = (memory_block_t*)((uint8_t*)ptr - sizeof(memory_block_t));
    block->is_free = 1;
    debug_print("DEBUG: Memory freed");
}
          </code></pre>

          <h3>2.4 Page Allocation</h3>
          <p>
            For many kernel operations—especially paging and MMU setup—we need one or more contiguous 4 KB pages. We simply wrap <code>kmalloc</code> to request <code>num_pages * 4096</code> bytes:
          </p>
          <pre><code>void* allocate_pages(size_t num_pages) {
    return kmalloc(num_pages * 4096);
}
          </code></pre>

          <h3>2.5 Page Table Copying</h3>
          <p>
            Upon process creation, the kernel clones the parent’s page tables into the child’s address space. In production OSes, copy-on-write reduces copying overhead; here we perform a direct memory copy of the first-level page directory (4 KB):
          </p>
          <pre><code>void copy_page_tables(uint32_t parent_cr3, uint32_t child_cr3) {
    // parent_cr3 and child_cr3 hold physical addresses of page directory
    copy_memory((void*)child_cr3, (void*)parent_cr3, 4096);
}

void copy_memory(void* dest, void* src, size_t size) {
    uint8_t* d = (uint8_t*)dest;
    uint8_t* s = (uint8_t*)src;
    for (size_t i = 0; i < size; i++) {
        d[i] = s[i];
    }
}
          </code></pre>
        </section>

        <!-- Future Directions -->
        <section>
          <h2>3. Future Directions</h2>
          <p>
            Our starting point prioritizes simplicity and clarity, but the real power comes as we evolve towards:
          </p>
          <ul>
            <li><strong>Free-list allocator:</strong> Reuse freed blocks to mitigate wasted space and fragmentation.</li>
            <li><strong>Buddy system:</strong> Efficiently split and merge power-of-two sized pages for predictable performance.</li>
            <li><strong>Copy-on-write:</strong> Delay page table duplication until a write occurs, reducing overhead on process creation.</li>
            <li><strong>Physical frame management:</strong> Maintain a bitmap to track free physical pages across the entire RAM.</li>
            <li><strong>Virtual memory and swapping:</strong> Extend to support demand paging and swap to disk under memory pressure.</li>
          </ul>
        </section>

        <section>
          <h2>4. Conclusion</h2>
          <p>
            This dual-pronged exploration detailed both the theoretical concepts behind bump allocation and the precise ApnaOS implementation. While rudimentary,
            these foundations offer a clear path to richer, more robust memory management as the project scales.
          </p>
        </section>

      </article>
    </main>

    <footer>
      © 2025 ApnaOS Team
    </footer>
  </div>
</body>
</html>
